{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8af715e",
   "metadata": {},
   "source": [
    "# Ejecutar un script de entrenamiento en Spark (Databrocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8060c9f",
   "metadata": {},
   "source": [
    "## Conectar a workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conectar\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
    "\n",
    "print(f\"Conectado al Workspace: {ml_client.workspace_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d23fc",
   "metadata": {},
   "source": [
    "## Conectar Databricks con Azure ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ff64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "databricks_id=\"/subscriptions/7decb7a4-f615-4cc3-9d7d-5de10998373f/resourceGroups/ntrgy-databrics/providers/Microsoft.Databricks/workspaces/natrgy-db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# Vincular el clúster de Databricks\n",
    "databricks_compute = AmlCompute(\n",
    "    name=\"my-databricks-compute\",\n",
    "    resource_id=databricks_id,\n",
    "    type=\"databricks\"\n",
    ")\n",
    "\n",
    "# Registrar el destino de computación\n",
    "ml_client.compute.begin_create_or_update(databricks_compute).result()\n",
    "print(f\"Databricks vinculado como destino de computación: {databricks_compute.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67d3de",
   "metadata": {},
   "source": [
    "## Entrenar un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create a folder for the script files\n",
    "script_folder = './src'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455042d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "# import libraries\n",
    "import mlflow\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pyspark.pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Inicializar Spark\n",
    "    # spark = SparkSession.builder.appName(\"DiabetesTraining\").getOrCreate()\n",
    "    # spark = SparkSession.getActiveSession()\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    if spark is None:\n",
    "        raise ValueError(\"No hay una sesión Spark activa disponible.\")\n",
    "\n",
    "    # read data\n",
    "    df = get_data(args.input_data, spark)\n",
    "\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "    # train model\n",
    "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # evaluate model\n",
    "    eval_model(model, X_test, y_test)\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(path, spark):\n",
    "    print(\"Reading data...\")\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function that splits the data\n",
    "def split_data(df):\n",
    "    print(\"Splitting data...\")\n",
    "    # (trainingData, testData) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "    # X_train, y_train = trainingData[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness',\n",
    "    # 'SerumInsulin','BMI','DiabetesPedigree','Age']].values, trainingData['Diabetic'].values\n",
    "    \n",
    "    # X_test, y_test = testData[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness',\n",
    "    # 'SerumInsulin','BMI','DiabetesPedigree','Age']].values, testData['Diabetic'].values\n",
    "    X, y = df[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness',\n",
    "    'SerumInsulin','BMI','DiabetesPedigree','Age']].values, df['Diabetic'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# function that trains the model\n",
    "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
    "    mlflow.log_param(\"Regularization rate\", reg_rate)\n",
    "    print(\"Training model...\")\n",
    "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "# function that evaluates the model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', acc)\n",
    "    mlflow.log_metric(\"training_accuracy_score\", acc)\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC: ' + str(auc))\n",
    "    mlflow.log_metric(\"AUC\", auc)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--input_data\", type=str, required=True, help=\"Path to the input dataset\")\n",
    "    parser.add_argument(\"--reg_rate\", dest='reg_rate', type=float, default=0.01)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaafe39",
   "metadata": {},
   "source": [
    "## Entorno de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/conda-env.yml\n",
    "name: sklearn-azure-ai-env\n",
    "channels:\n",
    "  - defaults\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - pip\n",
    "  - scikit-learn\n",
    "  - pandas\n",
    "  - numpy\n",
    "  - matplotlib\n",
    "  - pip:\n",
    "    - azure-ai-ml\n",
    "    - azure-identity\n",
    "    - mlflow\n",
    "    - mltable\n",
    "    - jsonschema\n",
    "    - pathlib\n",
    "    - pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "# Definir el entorno\n",
    "environment = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",  # Base image\n",
    "    conda_file=\"./src/conda-env.yml\"  # Archivo YAML con dependencias\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aab98e",
   "metadata": {},
   "source": [
    "## Job de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf34569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "# Definir el job\n",
    "job = command(\n",
    "    code=\"./src\",  # Carpeta que contiene el script\n",
    "    command=\"python train.py --input_data ${{inputs.input_data}} --reg_rate ${{inputs.reg_rate}}\",\n",
    "    environment=environment,\n",
    "    compute=\"my-databricks-compute\",  # Nombre del clúster Databricks\n",
    "    inputs={\n",
    "        \"input_data\": Input(type=\"uri_file\", path=\"azureml:diabetes-data-local-ric:1\"),\n",
    "        \"reg_rate\": 0.01,\n",
    "    },\n",
    "    display_name=\"train-diabetes-spark\",\n",
    "    experiment_name=\"diabetes-training-spark\",\n",
    ")\n",
    "\n",
    "# Enviar el job\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "print(f\"Job enviado: {returned_job.name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
