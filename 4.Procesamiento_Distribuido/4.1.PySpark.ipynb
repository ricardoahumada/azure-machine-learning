{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1506adab",
   "metadata": {},
   "source": [
    "# **Procesamiento Distribuido con PySpark en Pipelines en Azure ML**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd509b",
   "metadata": {},
   "source": [
    "## **Módulo 1: Introducción a PySpark (1 hora)**  \n",
    "**Objetivo:** Comprender qué es PySpark, cómo funciona y cuándo usarlo en comparación con herramientas como Pandas.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. ¿Qué es PySpark y cómo funciona?**\n",
    "- **Definición:**  \n",
    "  PySpark es la API de Python para Apache Spark, un motor de procesamiento distribuido diseñado para manejar grandes volúmenes de datos de manera eficiente.\n",
    "- **Funcionamiento:**  \n",
    "  - Utiliza un modelo de computación distribuida basado en clústeres.\n",
    "  - Divide los datos en particiones y procesa cada partición en paralelo en múltiples nodos.\n",
    "  - Proporciona abstracciones como RDDs (Resilient Distributed Datasets) y DataFrames para manipular datos.\n",
    "\n",
    "#### **1.1. Comparación con Pandas**\n",
    "| **Característica**       | **Pandas**                          | **PySpark**                         |\n",
    "|--------------------------|-------------------------------------|-------------------------------------|\n",
    "| **Escalabilidad**        | Limitado a datos en memoria local. | Escalable a grandes volúmenes de datos distribuidos. |\n",
    "| **Velocidad**            | Lento para grandes conjuntos de datos. | Rápido gracias al procesamiento paralelo. |\n",
    "| **Uso Recomendado**      | Datos pequeños o medianos.         | Grandes volúmenes de datos (Big Data). |\n",
    "\n",
    "- **Cuándo usar PySpark:**  \n",
    "  - Cuando el volumen de datos excede la capacidad de memoria local.\n",
    "  - Cuando necesitas procesar datos en tiempo real o realizar análisis distribuidos.\n",
    "  - Para integrar pipelines de machine learning con Big Data.\n",
    "\n",
    "#### **Referencias:**\n",
    "- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)\n",
    "- [PySpark vs Pandas Comparison](https://spark.apache.org/docs/latest/api/python/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af1bd9",
   "metadata": {},
   "source": [
    "## **Módulo 2: Configuración de PySpark en Azure (1 hora)**  \n",
    "**Objetivo:** Aprender a configurar PySpark en Azure utilizando Azure Databricks y conectarlo con Azure Blob Storage.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.1. Uso de Azure Databricks para Ejecutar PySpark**\n",
    "Azure Databricks es un servicio administrado que facilita la ejecución de trabajos de PySpark en clústeres escalables.\n",
    "\n",
    "- **Pasos para Configurar Azure Databricks:**\n",
    "  1. Crea un área de trabajo de Databricks en Azure Portal.\n",
    "  2. Configura un clúster de Spark en Databricks:\n",
    "     - Selecciona el número de nodos y el tipo de máquina virtual.\n",
    "     - Instala bibliotecas adicionales si es necesario (e.g., `azure-storage-blob`).\n",
    "  3. Conecta Databricks con Azure Machine Learning Workspace para integrarlo en pipelines.\n",
    "\n",
    "#### **Ejemplo Práctico: Creación de un Clúster en Databricks**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProcesamientoDistribuido\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Clúster de Spark inicializado.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2. Conexión con Azure Blob Storage**\n",
    "Azure Blob Storage es una solución ideal para almacenar grandes volúmenes de datos que se pueden procesar con PySpark.\n",
    "\n",
    "- **Pasos para Conectar Blob Storage:**\n",
    "  1. Configura una cuenta de almacenamiento en Azure Portal.\n",
    "  2. Usa la biblioteca `azure-storage-blob` para cargar/descargar datos.\n",
    "  3. Monta el Blob Storage en Databricks para acceso directo.\n",
    "\n",
    "#### **Ejemplo Práctico: Lectura de Datos desde Blob Storage**\n",
    "```python\n",
    "# Montar Blob Storage en Databricks\n",
    "dbutils.fs.mount(\n",
    "    source=\"wasbs://<container-name>@<storage-account>.blob.core.windows.net\",\n",
    "    mount_point=\"/mnt/mi-datos\",\n",
    "    extra_configs={\"fs.azure.account.key.<storage-account>.blob.core.windows.net\": \"<storage-key>\"}\n",
    ")\n",
    "\n",
    "# Leer datos desde Blob Storage\n",
    "df = spark.read.csv(\"/mnt/mi-datos/dataset.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "#### **Referencias:**\n",
    "- [Azure Databricks Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n",
    "- [Azure Blob Storage Integration](https://learn.microsoft.com/en-us/azure/storage/blobs/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7e4ca",
   "metadata": {},
   "source": [
    "## **Módulo 3: Operaciones Básicas con PySpark (2 horas)**  \n",
    "**Objetivo:** Aprender a cargar, transformar y analizar datos utilizando PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.1. Carga y Transformación de Datos**\n",
    "- **Carga de Datos:**  \n",
    "  PySpark soporta múltiples formatos como CSV, JSON, Parquet y más.\n",
    "  ```python\n",
    "  # Cargar un archivo CSV\n",
    "  df = spark.read.csv(\"datos.csv\", header=True, inferSchema=True)\n",
    "  ```\n",
    "\n",
    "- **Transformaciones Comunes:**  \n",
    "  - Selección de columnas:\n",
    "    ```python\n",
    "    df.select(\"columna1\", \"columna2\").show()\n",
    "    ```\n",
    "  - Filtrado de datos:\n",
    "    ```python\n",
    "    df.filter(df[\"edad\"] > 30).show()\n",
    "    ```\n",
    "  - Renombrar columnas:\n",
    "    ```python\n",
    "    df.withColumnRenamed(\"old_name\", \"new_name\").show()\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2. Agregaciones y Join de Tablas Distribuidas**\n",
    "- **Agregaciones:**  \n",
    "  Realiza operaciones como sumas, promedios y conteos.\n",
    "  ```python\n",
    "  # Calcular el promedio de una columna\n",
    "  df.groupBy(\"categoria\").agg({\"valor\": \"avg\"}).show()\n",
    "  ```\n",
    "\n",
    "- **Join de Tablas:**  \n",
    "  Combina múltiples tablas distribuidas.\n",
    "  ```python\n",
    "  # Unir dos DataFrames\n",
    "  df1.join(df2, df1.id == df2.id, \"inner\").show()\n",
    "  ```\n",
    "\n",
    "#### **Referencias:**\n",
    "- [PySpark DataFrame Operations](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5416bfa",
   "metadata": {},
   "source": [
    "## **Módulo 4: Integración con Pipelines (2 horas)**  \n",
    "**Objetivo:** Aprender a integrar PySpark en pipelines de Azure ML para preprocesar datos antes de entrenar modelos.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1. Preprocesamiento de Datos con PySpark**\n",
    "PySpark es ideal para preprocesar grandes volúmenes de datos antes de entrenar modelos en Azure ML.\n",
    "\n",
    "- **Pasos Comunes:**\n",
    "  1. Limpiar y normalizar los datos.\n",
    "  2. Realizar ingeniería de características.\n",
    "  3. Exportar los datos procesados para entrenamiento.\n",
    "\n",
    "#### **Ejemplo Práctico: Preprocesamiento con PySpark**\n",
    "```python\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Ingeniería de características\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Normalización\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_transformed)\n",
    "df_scaled = scaler_model.transform(df_transformed)\n",
    "\n",
    "df_scaled.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4.2. Ejemplo Práctico: Procesamiento de Datos Masivos para un Modelo de Clustering**\n",
    "Supongamos que queremos entrenar un modelo de clustering en Azure ML utilizando datos procesados con PySpark.\n",
    "\n",
    "#### **Pasos:**\n",
    "1. **Procesamiento con PySpark:**\n",
    "   - Limpia y transforma los datos.\n",
    "   - Exporta los datos procesados a Azure Blob Storage.\n",
    "2. **Entrenamiento del Modelo:**\n",
    "   - Carga los datos procesados en Azure ML.\n",
    "   - Entrena un modelo de clustering (e.g., K-Means).\n",
    "\n",
    "#### **Ejemplo Práctico: Pipeline Completo**\n",
    "```python\n",
    "# Paso 1: Procesamiento con PySpark\n",
    "df.write.format(\"parquet\").save(\"/mnt/mi-datos/procesados.parquet\")\n",
    "\n",
    "# Paso 2: Entrenamiento en Azure ML\n",
    "from azureml.core import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Cargar datos procesados\n",
    "dataset = Dataset.Tabular.from_parquet_files(path=(datastore, 'procesados.parquet'))\n",
    "X = dataset.to_pandas_dataframe()\n",
    "\n",
    "# Entrenar modelo\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(X)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe81658",
   "metadata": {},
   "source": [
    "## **Actividad Práctica (2 horas)**\n",
    "\n",
    "**Título:** Procesar un conjunto de datos grande con PySpark y entrenar un modelo de clustering en un pipeline de Azure ML.\n",
    "\n",
    "#### **Pasos:**\n",
    "1. **Configuración del Entorno:**\n",
    "   - Crea un clúster de Databricks en Azure.\n",
    "   - Monta un contenedor de Blob Storage en Databricks.\n",
    "\n",
    "2. **Procesamiento con PySpark:**\n",
    "   - Carga un conjunto de datos grande desde Blob Storage.\n",
    "   - Realiza limpieza, normalización y agregaciones con PySpark.\n",
    "   - Guarda los datos procesados en Blob Storage.\n",
    "\n",
    "3. **Entrenamiento del Modelo:**\n",
    "   - Carga los datos procesados en Azure ML.\n",
    "   - Entrena un modelo de clustering utilizando Scikit-learn o AutoML.\n",
    "\n",
    "4. **Despliegue del Modelo:**\n",
    "   - Despliega el modelo como un endpoint REST.\n",
    "   - Prueba el endpoint con datos de ejemplo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusión**\n",
    "Este curso proporciona una visión completa del uso de PySpark para procesamiento distribuido dentro de pipelines de Azure ML. Los participantes estarán preparados para manejar grandes volúmenes de datos y construir pipelines escalables para entrenar modelos de machine learning.\n",
    "\n",
    "#### **Referencias Generales:**\n",
    "- [Azure Databricks Documentation](https://learn.microsoft.com/en-us/azure/databricks/)\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Azure Machine Learning Documentation](https://learn.microsoft.com/en-us/azure/machine-learning/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
