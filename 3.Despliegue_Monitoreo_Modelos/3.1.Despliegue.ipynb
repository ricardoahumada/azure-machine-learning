{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b89f5f2",
   "metadata": {},
   "source": [
    "# **Despliegue de Modelos en Azure Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b44a1",
   "metadata": {},
   "source": [
    "\n",
    "### **1. Introducción al Despliegue de Modelos**\n",
    "El despliegue de un modelo es el proceso de exponerlo como un servicio web o API RESTful que pueda ser consumido por aplicaciones externas. Azure Machine Learning proporciona varias opciones para desplegar modelos, dependiendo del escenario y los requisitos de producción.\n",
    "\n",
    "#### **1.1. ¿Por qué es importante el despliegue?**\n",
    "- Permite integrar modelos de machine learning en aplicaciones empresariales.\n",
    "- Facilita la automatización de decisiones basadas en predicciones.\n",
    "- Proporciona escalabilidad para manejar grandes volúmenes de solicitudes.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Opciones de Despliegue en Azure ML**\n",
    "Azure ML ofrece múltiples opciones para desplegar modelos, cada una diseñada para diferentes necesidades y escenarios.\n",
    "\n",
    "#### **2.1. Azure Container Instances (ACI)**\n",
    "- **Descripción:**  \n",
    "  - Ideal para pruebas rápidas o despliegues pequeños.\n",
    "  - Se ejecuta en contenedores Docker sin necesidad de configurar infraestructura compleja.\n",
    "- **Cuándo usarlo:**  \n",
    "  - Pruebas iniciales del modelo.\n",
    "  - Escenarios con bajo tráfico o carga reducida.\n",
    "- **Ventajas:**  \n",
    "  - Fácil de configurar.\n",
    "  - Costo bajo para pruebas rápidas.\n",
    "- **Ejemplo Práctico: Despliegue en ACI**\n",
    "  ```python\n",
    "  from azureml.core.webservice import AciWebservice\n",
    "  from azureml.core.model import InferenceConfig\n",
    "\n",
    "  inference_config = InferenceConfig(entry_script=\"score.py\")\n",
    "  deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "  service = Model.deploy(workspace=ws,\n",
    "                         name=\"mi-servicio-aci\",\n",
    "                         models=[model],\n",
    "                         inference_config=inference_config,\n",
    "                         deployment_config=deployment_config)\n",
    "  service.wait_for_deployment(show_output=True)\n",
    "  print(f\"Endpoint disponible en: {service.scoring_uri}\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.2. Azure Kubernetes Service (AKS)**\n",
    "- **Descripción:**  \n",
    "  - Diseñado para entornos de producción con alta demanda.\n",
    "  - Ofrece escalabilidad, balanceo de carga y monitoreo avanzado.\n",
    "- **Cuándo usarlo:**  \n",
    "  - Despliegues en producción con alto tráfico.\n",
    "  - Escenarios que requieren escalabilidad automática.\n",
    "- **Ventajas:**  \n",
    "  - Soporte para múltiples réplicas.\n",
    "  - Integración con herramientas de monitoreo y alertas.\n",
    "- **Ejemplo Práctico: Despliegue en AKS**\n",
    "  ```python\n",
    "  from azureml.core.compute import AksCompute\n",
    "  from azureml.core.webservice import AksWebservice\n",
    "\n",
    "  # Crear un clúster AKS (si no existe)\n",
    "  aks_target = AksCompute(workspace=ws, name=\"mi-cluster-aks\")\n",
    "\n",
    "  # Configurar el despliegue\n",
    "  deployment_config = AksWebservice.deploy_configuration(cpu_cores=2, memory_gb=4)\n",
    "\n",
    "  service = Model.deploy(workspace=ws,\n",
    "                         name=\"mi-servicio-aks\",\n",
    "                         models=[model],\n",
    "                         inference_config=inference_config,\n",
    "                         deployment_config=deployment_config,\n",
    "                         deployment_target=aks_target)\n",
    "  service.wait_for_deployment(show_output=True)\n",
    "  print(f\"Endpoint disponible en: {service.scoring_uri}\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.3. Azure Functions**\n",
    "- **Descripción:**  \n",
    "  - Ideal para despliegues serverless y eventos disparados por HTTP.\n",
    "  - Ejecuta el modelo solo cuando se recibe una solicitud.\n",
    "- **Cuándo usarlo:**  \n",
    "  - Casos de uso con baja frecuencia de solicitudes.\n",
    "  - Escenarios donde el costo es una preocupación.\n",
    "- **Ventajas:**  \n",
    "  - Bajo costo debido al modelo serverless.\n",
    "  - Respuesta rápida para eventos puntuales.\n",
    "- **Ejemplo Práctico: Despliegue en Azure Functions**\n",
    "  ```python\n",
    "  from azureml.contrib.functions import package\n",
    "  from azureml.core.model import InferenceConfig\n",
    "\n",
    "  inference_config = InferenceConfig(entry_script=\"score.py\")\n",
    "  package = package(ws, [model], inference_config)\n",
    "  package.wait_for_creation(show_output=True)\n",
    "  print(f\"Paquete Azure Functions disponible en: {package.get_deploy_status()}\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.4. IoT Edge**\n",
    "- **Descripción:**  \n",
    "  - Diseñado para despliegues en dispositivos periféricos.\n",
    "  - Ejecuta el modelo directamente en dispositivos IoT.\n",
    "- **Cuándo usarlo:**  \n",
    "  - Escenarios de edge computing.\n",
    "  - Aplicaciones que requieren latencia ultrabaja.\n",
    "- **Ventajas:**  \n",
    "  - Reducción de la latencia al procesar datos localmente.\n",
    "  - Ahorro de ancho de banda al evitar enviar datos al cloud.\n",
    "- **Ejemplo Práctico: Despliegue en IoT Edge**\n",
    "  ```python\n",
    "  from azureml.core.model import Model\n",
    "\n",
    "  model = Model.register(workspace=ws,\n",
    "                         model_path=\"model.pkl\",\n",
    "                         model_name=\"mi-modelo\")\n",
    "\n",
    "  # Generar un paquete para IoT Edge\n",
    "  package = Model.package(ws, [model], image_name=\"mi-modelo-iot-edge\")\n",
    "  package.wait_for_creation(show_output=True)\n",
    "  print(f\"Paquete IoT Edge disponible en: {package.get_deploy_status()}\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Pruebas de API**\n",
    "Una vez desplegado el modelo, es crucial probar el endpoint REST para asegurarse de que funcione correctamente.\n",
    "\n",
    "#### **3.1. Herramientas de Prueba**\n",
    "- **Postman:**  \n",
    "  - Una herramienta popular para probar APIs RESTful.\n",
    "  - Permite enviar solicitudes HTTP y visualizar las respuestas.\n",
    "- **Requests en Python:**  \n",
    "  - Biblioteca de Python para realizar solicitudes HTTP.\n",
    "\n",
    "#### **3.2. Ejemplo Práctico: Prueba con Requests**\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://<endpoint-url>/score\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\"data\": [[5.1, 3.5, 1.4, 0.2]]}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Ejercicio Práctico**\n",
    "\n",
    "**Título:** Desplegar un modelo de clasificación en Azure ML y probar su endpoint REST.\n",
    "\n",
    "#### **Pasos:**\n",
    "1. **Entrenamiento del Modelo:**\n",
    "   - Entrena un modelo de clasificación utilizando Scikit-learn.\n",
    "   ```python\n",
    "   from sklearn.datasets import load_iris\n",
    "   from sklearn.linear_model import LogisticRegression\n",
    "   import joblib\n",
    "\n",
    "   X, y = load_iris(return_X_y=True)\n",
    "   model = LogisticRegression()\n",
    "   model.fit(X, y)\n",
    "   joblib.dump(model, \"model.pkl\")\n",
    "   ```\n",
    "\n",
    "2. **Despliegue en ACI:**\n",
    "   - Despliega el modelo en Azure Container Instances.\n",
    "   ```python\n",
    "   from azureml.core.webservice import AciWebservice\n",
    "   from azureml.core.model import InferenceConfig\n",
    "\n",
    "   inference_config = InferenceConfig(entry_script=\"score.py\")\n",
    "   deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "   service = Model.deploy(workspace=ws,\n",
    "                          name=\"mi-servicio-aci\",\n",
    "                          models=[\"model.pkl\"],\n",
    "                          inference_config=inference_config,\n",
    "                          deployment_config=deployment_config)\n",
    "   service.wait_for_deployment(show_output=True)\n",
    "   print(f\"Endpoint disponible en: {service.scoring_uri}\")\n",
    "   ```\n",
    "\n",
    "3. **Prueba del Endpoint:**\n",
    "   - Usa Postman o `requests` para probar el endpoint.\n",
    "   ```python\n",
    "   import requests\n",
    "   import json\n",
    "\n",
    "   url = \"http://<endpoint-url>/score\"\n",
    "   headers = {\"Content-Type\": \"application/json\"}\n",
    "   data = {\"data\": [[5.1, 3.5, 1.4, 0.2]]}\n",
    "\n",
    "   response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "   print(response.json())\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusión**\n",
    "Este módulo proporciona una visión completa de las opciones de despliegue en Azure ML, desde pruebas rápidas con ACI hasta despliegues robustos con AKS. Los participantes estarán preparados para elegir la opción adecuada según sus necesidades y desplegar modelos de manera efectiva.\n",
    "\n",
    "#### **Referencias Generales:**\n",
    "- [Azure ML Deployment Documentation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where)\n",
    "- [Postman Documentation](https://learning.postman.com/docs/getting-started/introduction/)\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
